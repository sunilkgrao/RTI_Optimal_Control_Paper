#!/usr/bin/env python3
"""
Section III.F Growth Rate Extractor
Implements the exact protocol described in the paper for Œ≥(k) extraction
with RANSAC fitting, universal collapse normalization, and CI propagation
"""

import numpy as np
import h5py
from scipy import fft, optimize, stats
from scipy.signal import find_peaks
from sklearn.linear_model import RANSACRegressor, HuberRegressor
import pandas as pd
import matplotlib.pyplot as plt
import logging
from pathlib import Path
import yaml

class SectionIIIFExtractor:
    """
    Growth rate extractor implementing the exact Section III.F protocol
    """
    
    def __init__(self, output_dir='../analysis'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger('SectionIIIF')
        
        # Protocol parameters from paper
        self.linearity_threshold = 0.95  # R¬≤ threshold for linear window
        self.huber_epsilon = 1.35        # Huber regression parameter
        self.ransac_min_samples = 0.6    # Minimum inlier fraction
        self.failure_mode_kd_threshold = 0.3  # kd ratio threshold
        
    def load_pic_diagnostics(self, sim_path, probe_file=None):
        """Load SMILEI diagnostic data"""
        
        sim_path = Path(sim_path)
        
        # Load probe data (interface tracking)
        if probe_file:
            probe_path = sim_path / probe_file
        else:
            probe_files = list(sim_path.glob("Probes*.h5"))
            if not probe_files:
                raise FileNotFoundError(f"No probe files found in {sim_path}")
            probe_path = probe_files[0]
        
        self.logger.info(f"Loading probe data from {probe_path}")
        
        with h5py.File(probe_path, 'r') as f:
            # Extract time array
            times = f['data/0/probe_0'].attrs['time']
            
            # Extract spatial coordinates
            y_coords = f['data/0/probe_0/y'][:]
            
            # Extract electron density over time
            rho_e_data = []
            for i, t in enumerate(times):
                dataset_name = f'data/{i}/probe_0/Rho_electrons'
                if dataset_name in f:
                    rho_e_data.append(f[dataset_name][:])
                else:
                    self.logger.warning(f"Missing data at time {t}")
            
            rho_e_data = np.array(rho_e_data)
        
        diagnostic_data = {
            'times': times,
            'y_coords': y_coords,
            'rho_electrons': rho_e_data,
            'metadata': {
                'source_file': str(probe_path),
                'n_timesteps': len(times),
                'n_spatial_points': len(y_coords)
            }
        }
        
        self.logger.info(f"Loaded diagnostics: {len(times)} timesteps, {len(y_coords)} spatial points")
        return diagnostic_data
    
    def extract_interface_perturbation(self, diagnostic_data, ne_background=None):
        """Extract interface perturbation Œ∑(y,t) from density data"""
        
        times = diagnostic_data['times']
        y_coords = diagnostic_data['y_coords']
        rho_e = diagnostic_data['rho_electrons']
        
        if ne_background is None:
            ne_background = np.max(rho_e) * 0.5  # 50% density contour
        
        interface_positions = []
        
        for t_idx, t in enumerate(times):
            rho_profile = rho_e[t_idx, :]
            
            # Find interface position (where density crosses threshold)
            interface_pos = self._find_interface_contour(y_coords, rho_profile, ne_background)
            interface_positions.append(interface_pos)
        
        interface_positions = np.array(interface_positions)
        
        # Extract perturbation by subtracting mean position
        eta_data = []
        for t_idx in range(len(times)):
            mean_pos = np.mean(interface_positions[t_idx])
            eta = interface_positions[t_idx] - mean_pos
            eta_data.append(eta)
        
        eta_data = np.array(eta_data)
        
        perturbation_data = {
            'times': times,
            'y_coords': y_coords,
            'eta': eta_data,  # Shape: (n_times, n_y)
            'interface_positions': interface_positions,
            'ne_threshold': ne_background
        }
        
        self.logger.info(f"Extracted interface perturbation: {eta_data.shape}")
        return perturbation_data
    
    def _find_interface_contour(self, y_coords, rho_profile, threshold):
        """Find interface position along each y coordinate"""
        
        # Simple threshold crossing method
        # In real implementation, would use more sophisticated contour finding
        interface_pos = np.zeros_like(y_coords)
        
        for i in range(len(y_coords)):
            # For now, use density gradient to estimate interface position
            # This is a simplified version - real implementation would be more robust
            interface_pos[i] = np.sum(rho_profile > threshold * 0.5) * (y_coords[1] - y_coords[0])
        
        return interface_pos
    
    def extract_spectral_growth_rates(self, perturbation_data, k_modes=None):
        """Extract Œ≥(k) using Section III.F protocol"""
        
        times = perturbation_data['times']
        eta = perturbation_data['eta']
        y_coords = perturbation_data['y_coords']
        
        # FFT to get k-space amplitudes
        dy = y_coords[1] - y_coords[0]
        k_values = 2 * np.pi * fft.fftfreq(len(y_coords), dy)
        k_positive = k_values[k_values > 0]
        
        if k_modes is None:
            # Select k modes based on resolution and physical constraints
            k_modes = k_positive[k_positive < np.max(k_positive) * 0.5]  # Avoid Nyquist
        
        growth_rate_results = {}
        
        for k in k_modes:
            k_idx = np.argmin(np.abs(k_values - k))
            
            # Extract temporal evolution of this k-mode
            eta_k_evolution = []
            for t_idx in range(len(times)):
                eta_fft = fft.fft(eta[t_idx, :])
                eta_k_evolution.append(np.abs(eta_fft[k_idx]))
            
            eta_k_evolution = np.array(eta_k_evolution)
            
            # Extract growth rate using Section III.F protocol
            gamma_result = self._extract_gamma_single_mode(times, eta_k_evolution, k)
            
            if gamma_result['valid']:
                growth_rate_results[k] = gamma_result
        
        self.logger.info(f"Extracted growth rates for {len(growth_rate_results)} k-modes")
        return growth_rate_results
    
    def _extract_gamma_single_mode(self, times, eta_k_amplitude, k_value):
        """Extract growth rate for single k-mode using RANSAC/Huber fitting"""
        
        # Step 1: Find linear growth window
        log_amplitude = np.log(eta_k_amplitude + 1e-12)  # Avoid log(0)
        finite_mask = np.isfinite(log_amplitude)
        
        if np.sum(finite_mask) < 4:
            return {'valid': False, 'reason': 'insufficient_data'}
        
        times_clean = times[finite_mask]
        log_amp_clean = log_amplitude[finite_mask]
        
        # Sliding window to find best linear regime
        linear_window = self._find_optimal_linear_window(times_clean, log_amp_clean)
        
        if linear_window is None:
            return {'valid': False, 'reason': 'no_linear_window'}
        
        t_fit = times_clean[linear_window]
        log_fit = log_amp_clean[linear_window]
        
        # Step 2: RANSAC fitting with Huber loss
        ransac = RANSACRegressor(
            base_estimator=HuberRegressor(epsilon=self.huber_epsilon),
            min_samples=max(2, int(len(t_fit) * self.ransac_min_samples)),
            residual_threshold=0.1,
            random_state=42
        )
        
        try:
            ransac.fit(t_fit.reshape(-1, 1), log_fit)
            gamma = ransac.estimator_.coef_[0]
            gamma_intercept = ransac.estimator_.intercept_
            
            # Step 3: Uncertainty estimation
            inlier_mask = ransac.inlier_mask_
            t_inliers = t_fit[inlier_mask]
            log_inliers = log_fit[inlier_mask]
            
            # Linear regression on inliers for CI
            slope, intercept, r_value, p_value, std_err = stats.linregress(t_inliers, log_inliers)
            
            gamma_ci = 1.96 * std_err  # 95% CI
            
            result = {
                'valid': True,
                'k_value': k_value,
                'growth_rate': gamma,
                'growth_rate_ci': gamma_ci,
                'r_squared': r_value**2,
                'p_value': p_value,
                'n_points': len(t_fit),
                'n_inliers': np.sum(inlier_mask),
                'inlier_fraction': np.sum(inlier_mask) / len(t_fit),
                'linear_window': linear_window,
                'extraction_method': 'Section_IIIF_RANSAC'
            }
            
            return result
            
        except Exception as e:
            return {'valid': False, 'reason': f'fitting_failed: {e}'}
    
    def _find_optimal_linear_window(self, times, log_amplitudes):
        """Find optimal linear window using sliding R¬≤ maximization"""
        
        n_points = len(times)
        min_window_size = max(4, n_points // 4)
        
        best_r_squared = -1
        best_window = None
        
        for start_idx in range(n_points - min_window_size):
            for end_idx in range(start_idx + min_window_size, n_points + 1):
                
                t_window = times[start_idx:end_idx]
                log_window = log_amplitudes[start_idx:end_idx]
                
                if len(t_window) < 3:
                    continue
                
                # Linear regression
                slope, intercept, r_value, p_value, std_err = stats.linregress(t_window, log_window)
                r_squared = r_value**2
                
                if r_squared > best_r_squared and r_squared > self.linearity_threshold:
                    best_r_squared = r_squared
                    best_window = slice(start_idx, end_idx)
        
        if best_window and best_r_squared > self.linearity_threshold:
            return best_window
        else:
            return None
    
    def normalize_for_universal_collapse(self, growth_rate_results, physical_params):
        """Normalize Œ≥(k) data for universal collapse following Section III.F"""
        
        # Extract physical parameters
        laser_params = physical_params.get('laser', {})
        target_params = physical_params.get('target', {})
        
        a0 = laser_params.get('a0', 10.0)
        lambda0 = laser_params.get('wavelength_m', 0.8e-6)
        ne_nc = target_params.get('ne_over_nc', 100.0)
        thickness = target_params.get('thickness_m', 50e-9)
        
        # Compute scales
        c = 299792458.0
        w0 = 2 * np.pi * c / lambda0
        nc = 1.1e21 / (lambda0 * 1e6)**2  # m‚Åª¬≥
        ne = ne_nc * nc
        
        # Thermal scale
        k_thermal = np.sqrt(ne * 1.602e-19**2 / (8.854e-12 * 1.381e-23 * 300 * 11604.5))
        
        normalized_results = {}
        
        for k, result in growth_rate_results.items():
            if not result['valid']:
                continue
            
            gamma = result['growth_rate']
            gamma_ci = result['growth_rate_ci']
            
            # Normalization following Section III.F
            # x = k / k_T, y = Œ≥ / (a * k_T)
            # where a is the effective acceleration and k_T is thermal scale
            
            a_eff = a0**2 * w0**2 / c  # Effective acceleration from ponderomotive force
            
            x_normalized = k / k_thermal
            y_normalized = gamma / (a_eff * k_thermal)
            y_error_normalized = gamma_ci / (a_eff * k_thermal)
            
            # Check validity flags
            kd_ratio = k * thickness
            validity_flags = {
                'kd_thin_foil': kd_ratio < self.failure_mode_kd_threshold,
                'sufficient_inliers': result['inlier_fraction'] > 0.5,
                'good_linearity': result['r_squared'] > self.linearity_threshold
            }
            
            normalized_results[k] = {
                'k_original': k,
                'x_normalized': x_normalized,
                'y_normalized': y_normalized,
                'y_error_normalized': y_error_normalized,
                'gamma_original': gamma,
                'gamma_ci_original': gamma_ci,
                'kd_ratio': kd_ratio,
                'validity_flags': validity_flags,
                'valid_for_collapse': all(validity_flags.values())
            }
        
        self.logger.info(f"Normalized {len(normalized_results)} modes for universal collapse")
        return normalized_results
    
    def fit_universal_collapse_parameters(self, normalized_results):
        """Fit (k_T, Œ¶‚ÇÉ) parameters from universal collapse data"""
        
        # Extract valid points
        valid_points = [(k, data) for k, data in normalized_results.items() 
                       if data['valid_for_collapse']]
        
        if len(valid_points) < 3:
            return {'valid': False, 'reason': 'insufficient_valid_points'}
        
        # Prepare data
        x_data = np.array([data['x_normalized'] for _, data in valid_points])
        y_data = np.array([data['y_normalized'] for _, data in valid_points])
        y_errors = np.array([data['y_error_normalized'] for _, data in valid_points])
        
        # Universal function G(x; Œ¶‚ÇÉ) - simplified model
        def universal_function(x, phi_3):
            """Universal collapse function G(x; Œ¶‚ÇÉ)"""
            # Simplified model: G(x) = x * exp(-phi_3 * x¬≤)
            return x * np.exp(-phi_3 * x**2)
        
        # Fit using weighted least squares
        try:
            popt, pcov = optimize.curve_fit(
                universal_function, 
                x_data, 
                y_data,
                sigma=y_errors,
                p0=[1.0],  # Initial guess for Œ¶‚ÇÉ
                bounds=([0.1], [10.0])
            )
            
            phi_3_fitted = popt[0]
            phi_3_error = np.sqrt(pcov[0, 0])
            
            # Compute goodness of fit
            y_pred = universal_function(x_data, phi_3_fitted)
            r_squared = 1 - np.sum((y_data - y_pred)**2) / np.sum((y_data - np.mean(y_data))**2)
            
            collapse_parameters = {
                'valid': True,
                'phi_3': phi_3_fitted,
                'phi_3_error': phi_3_error,
                'r_squared': r_squared,
                'n_points': len(valid_points),
                'fitting_method': 'weighted_least_squares'
            }
            
            self.logger.info(f"Universal collapse fit: Œ¶‚ÇÉ = {phi_3_fitted:.3f} ¬± {phi_3_error:.3f}")
            return collapse_parameters
            
        except Exception as e:
            return {'valid': False, 'reason': f'fitting_failed: {e}'}
    
    def create_analysis_report(self, sim_path, growth_rate_results, normalized_results, 
                             collapse_parameters, physical_params):
        """Create comprehensive analysis report"""
        
        # Generate plots
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # Plot 1: Raw growth rates Œ≥(k)
        ax1 = axes[0, 0]
        valid_results = {k: v for k, v in growth_rate_results.items() if v['valid']}
        
        if valid_results:
            k_vals = list(valid_results.keys())
            gamma_vals = [v['growth_rate'] for v in valid_results.values()]
            gamma_errs = [v['growth_rate_ci'] for v in valid_results.values()]
            
            ax1.errorbar(k_vals, gamma_vals, yerr=gamma_errs, 
                        marker='o', linestyle='-', capsize=3)
            ax1.set_xlabel('Wavenumber k [1/m]')
            ax1.set_ylabel('Growth Rate Œ≥ [1/s]')
            ax1.set_title('Raw Growth Rates')
            ax1.grid(True, alpha=0.3)
        
        # Plot 2: Universal collapse coordinates
        ax2 = axes[0, 1]
        valid_collapse = {k: v for k, v in normalized_results.items() if v['valid_for_collapse']}
        
        if valid_collapse:
            x_norm = [v['x_normalized'] for v in valid_collapse.values()]
            y_norm = [v['y_normalized'] for v in valid_collapse.values()]
            y_err_norm = [v['y_error_normalized'] for v in valid_collapse.values()]
            
            ax2.errorbar(x_norm, y_norm, yerr=y_err_norm,
                        marker='s', linestyle='', capsize=3, alpha=0.7)
            
            # Plot fitted universal function
            if collapse_parameters['valid']:
                x_theory = np.linspace(min(x_norm), max(x_norm), 100)
                phi_3 = collapse_parameters['phi_3']
                y_theory = x_theory * np.exp(-phi_3 * x_theory**2)
                ax2.plot(x_theory, y_theory, 'r--', linewidth=2, 
                        label=f'G(x; Œ¶‚ÇÉ={phi_3:.3f})')
                ax2.legend()
            
            ax2.set_xlabel('Normalized Wavenumber x = k/k_T')
            ax2.set_ylabel('Normalized Growth Rate y = Œ≥/(ak_T)')
            ax2.set_title('Universal Collapse')
            ax2.grid(True, alpha=0.3)
        
        # Plot 3: Validity checks
        ax3 = axes[1, 0]
        
        kd_ratios = [v['kd_ratio'] for v in normalized_results.values()]
        r_squared_vals = [growth_rate_results[k]['r_squared'] for k in normalized_results.keys() 
                         if growth_rate_results[k]['valid']]
        
        ax3.scatter(kd_ratios, r_squared_vals, alpha=0.7)
        ax3.axvline(x=0.3, color='red', linestyle='--', label='kd threshold')
        ax3.axhline(y=0.95, color='red', linestyle='--', label='R¬≤ threshold')
        ax3.set_xlabel('kd Ratio')
        ax3.set_ylabel('Linear Fit R¬≤')
        ax3.set_title('Validity Diagnostics')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Plot 4: Analysis summary
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        summary_text = f"""
        SECTION III.F ANALYSIS RESULTS
        
        Source: {sim_path.name}
        
        Growth Rate Extraction:
        ‚Ä¢ Total k-modes analyzed: {len(growth_rate_results)}
        ‚Ä¢ Valid extractions: {len([v for v in growth_rate_results.values() if v['valid']])}
        ‚Ä¢ Mean R¬≤: {np.mean([v['r_squared'] for v in growth_rate_results.values() if v['valid']]):.3f}
        
        Universal Collapse:
        ‚Ä¢ Valid points: {len(valid_collapse)} / {len(normalized_results)}
        ‚Ä¢ Œ¶‚ÇÉ parameter: {collapse_parameters.get('phi_3', 'N/A'):.3f} ¬± {collapse_parameters.get('phi_3_error', 0):.3f}
        ‚Ä¢ Collapse R¬≤: {collapse_parameters.get('r_squared', 'N/A'):.3f}
        
        Validation Status:
        {'‚úÖ PASSED' if collapse_parameters.get('valid', False) else '‚ùå FAILED'}
        """
        
        ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes,
                fontsize=10, verticalalignment='top',
                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))
        
        plt.tight_layout()
        
        # Save report
        report_file = self.output_dir / f'{sim_path.name}_section_iiif_report.png'
        plt.savefig(report_file, dpi=150, bbox_inches='tight')
        
        return report_file

def test_section_iiif_extractor():
    """Test the Section III.F extractor with synthetic data"""
    
    extractor = SectionIIIFExtractor()
    
    # Create synthetic PIC-like data
    times = np.linspace(0, 1e-12, 100)  # 100 timesteps over 1 ps
    y_coords = np.linspace(0, 20e-6, 200)  # 20 Œºm domain
    
    # Synthetic RTI growth with single mode
    k_seed = 2 * np.pi / (5e-6)  # 5 Œºm wavelength
    gamma_true = 5e12  # 5 THz growth rate
    
    synthetic_data = {
        'times': times,
        'y_coords': y_coords,
        'eta': np.zeros((len(times), len(y_coords)))
    }
    
    # Generate synthetic Œ∑(y,t) with exponential growth
    for i, t in enumerate(times):
        amplitude = 1e-9 * np.exp(gamma_true * t)  # Start with 1 nm amplitude
        synthetic_data['eta'][i, :] = amplitude * np.sin(k_seed * y_coords)
        
        # Add noise
        noise = np.random.normal(0, amplitude * 0.1, len(y_coords))
        synthetic_data['eta'][i, :] += noise
    
    # Extract growth rates
    growth_results = extractor.extract_spectral_growth_rates(
        synthetic_data, 
        k_modes=[k_seed]
    )
    
    # Physical parameters for normalization
    phys_params = {
        'laser': {'a0': 15.0, 'wavelength_m': 0.8e-6},
        'target': {'ne_over_nc': 100.0, 'thickness_m': 50e-9}
    }
    
    normalized = extractor.normalize_for_universal_collapse(growth_results, phys_params)
    collapse_fit = extractor.fit_universal_collapse_parameters(normalized)
    
    print("üî¨ SECTION III.F EXTRACTOR TEST")
    print("=" * 40)
    print(f"True growth rate: {gamma_true:.2e} Hz")
    
    if growth_results and k_seed in growth_results:
        extracted = growth_results[k_seed]
        if extracted['valid']:
            print(f"Extracted growth rate: {extracted['growth_rate']:.2e} ¬± {extracted['growth_rate_ci']:.2e} Hz")
            print(f"Relative error: {abs(extracted['growth_rate'] - gamma_true)/gamma_true:.1%}")
            print(f"R¬≤: {extracted['r_squared']:.4f}")
        else:
            print(f"Extraction failed: {extracted['reason']}")
    
    if collapse_fit['valid']:
        print(f"Universal collapse Œ¶‚ÇÉ: {collapse_fit['phi_3']:.3f} ¬± {collapse_fit['phi_3_error']:.3f}")
    
    return extractor, growth_results, normalized, collapse_fit

if __name__ == "__main__":
    test_section_iiif_extractor()